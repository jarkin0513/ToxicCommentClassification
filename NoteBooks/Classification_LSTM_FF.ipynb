{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libararies\n"
      ],
      "metadata": {
        "id": "rAwseUUkLuQz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR610yuoF2hK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to drive\n",
        "\n",
        "extract and inspect data set\n"
      ],
      "metadata": {
        "id": "MT2odifvL12v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls '/content/drive/MyDrive/jigsaw-toxic-comment-classification-challenge'\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqskAbEGJNph",
        "outputId": "14120695-cc2e-4209-f47f-216731664953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "sample_submission.csv.zip  test.csv.zip  test_labels.csv.zip  train.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the zip files in Drive\n",
        "zip_dir = '/content/drive/MyDrive/jigsaw-toxic-comment-classification-challenge/'\n",
        "\n",
        "# Unzip train.csv.zip\n",
        "with zipfile.ZipFile(zip_dir + 'train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Unzip test.csv.zip\n",
        "with zipfile.ZipFile(zip_dir + 'test.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Load the CSVs\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "G_LyhYiUKFeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and inspect data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print(\"Training dataset shape:\", train_df.shape)\n",
        "print(\"Test dataset shape:\", test_df.shape)\n",
        "\n",
        "print(\"\\nFirst 5 rows of the training data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "\n",
        "print(\"\\nTraining dataset info:\")\n",
        "print(train_df.info())\n",
        "\n",
        "\n",
        "print(\"\\nMissing values in the training data:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WKgUoNayHK0d",
        "outputId": "187bf751-db3c-43d8-f0f1-00561d46de1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset shape: (159571, 8)\n",
            "Test dataset shape: (153164, 2)\n",
            "\n",
            "First 5 rows of the training data:\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n",
            "\n",
            "Training dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   id             159571 non-null  object\n",
            " 1   comment_text   159571 non-null  object\n",
            " 2   toxic          159571 non-null  int64 \n",
            " 3   severe_toxic   159571 non-null  int64 \n",
            " 4   obscene        159571 non-null  int64 \n",
            " 5   threat         159571 non-null  int64 \n",
            " 6   insult         159571 non-null  int64 \n",
            " 7   identity_hate  159571 non-null  int64 \n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 9.7+ MB\n",
            "None\n",
            "\n",
            "Missing values in the training data:\n",
            "id               0\n",
            "comment_text     0\n",
            "toxic            0\n",
            "severe_toxic     0\n",
            "obscene          0\n",
            "threat           0\n",
            "insult           0\n",
            "identity_hate    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "print(train_df[label_cols].sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "846tUy7rK5s1",
        "outputId": "34783785-b548-4b2f-ac01-c3e3d2a21df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxic            15294\n",
            "severe_toxic      1595\n",
            "obscene           8449\n",
            "threat             478\n",
            "insult            7877\n",
            "identity_hate     1405\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning\n"
      ],
      "metadata": {
        "id": "j9Gs44ahLsb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "  # Remove email addresses\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "  # Remove special characters and numbers\n",
        "  text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "  # Remove extra spacing\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "AwNr_eYAMIBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['clean_comment'] = train_df['comment_text'].apply(clean_text)\n",
        "print(train_df[['comment_text', 'clean_comment']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_mYvz1vOljK",
        "outputId": "b7049b7e-6456-4728-bc64-6988ff3d4983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        comment_text  \\\n",
            "0  Explanation\\nWhy the edits made under my usern...   \n",
            "1  D'aww! He matches this background colour I'm s...   \n",
            "2  Hey man, I'm really not trying to edit war. It...   \n",
            "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
            "4  You, sir, are my hero. Any chance you remember...   \n",
            "\n",
            "                                       clean_comment  \n",
            "0  explanation why the edits made under my userna...  \n",
            "1  daww he matches this background colour im seem...  \n",
            "2  hey man im really not trying to edit war its j...  \n",
            "3  more i cant make any real suggestions on impro...  \n",
            "4  you sir are my hero any chance you remember wh...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "remove stop and rare words"
      ],
      "metadata": {
        "id": "xIsUammRRWcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_data(text):\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  return filtered_tokens\n",
        "\n",
        "train_df['tokens'] = train_df['clean_comment'].apply(tokenize_data)\n",
        "print(train_df[['comment_text', 'tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FKgUAphoRVxv",
        "outputId": "7c705f5b-0e86-462a-f6f9-c7cb38d683a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        comment_text  \\\n",
            "0  Explanation\\nWhy the edits made under my usern...   \n",
            "1  D'aww! He matches this background colour I'm s...   \n",
            "2  Hey man, I'm really not trying to edit war. It...   \n",
            "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
            "4  You, sir, are my hero. Any chance you remember...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [explanation, edits, made, username, hardcore,...  \n",
            "1  [daww, matches, background, colour, im, seemin...  \n",
            "2  [hey, man, im, really, trying, edit, war, guy,...  \n",
            "3  [cant, make, real, suggestions, improvement, w...  \n",
            "4         [sir, hero, chance, remember, page, thats]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(token_lists, min_freq=5):\n",
        "  freq = Counter()\n",
        "  for tokens in token_lists:\n",
        "    freq.update(tokens)\n",
        "\n",
        "  vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "\n",
        "  index = 2\n",
        "  for token, count in freq.items():\n",
        "    if count >= min_freq:\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "\n",
        "  return vocab\n",
        "\n",
        "\n",
        "# Use a subset of data during development\n",
        "train_subset = train_df\n",
        "\n",
        "\n",
        "\n",
        "vocab = build_vocab(train_subset['tokens'], min_freq= 3)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "train_subset['token_ids'] = train_subset['tokens'].apply(lambda tokens: [vocab.get(token, vocab['<UNK>']) for token in tokens])\n",
        "\n",
        "print(train_subset[['tokens', 'token_ids']].head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TYa8aQcoWHN",
        "outputId": "5db7ae31-889f-40d7-cf8c-4849c5042412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 62476\n",
            "                                              tokens  \\\n",
            "0  [explanation, edits, made, username, hardcore,...   \n",
            "1  [daww, matches, background, colour, im, seemin...   \n",
            "2  [hey, man, im, really, trying, edit, war, guy,...   \n",
            "3  [cant, make, real, suggestions, improvement, w...   \n",
            "4         [sir, hero, chance, remember, page, thats]   \n",
            "\n",
            "                                           token_ids  \n",
            "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
            "1        [1, 28, 29, 30, 26, 31, 32, 33, 23, 34, 35]  \n",
            "2  [36, 37, 26, 38, 39, 40, 41, 42, 43, 44, 45, 4...  \n",
            "3  [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 6...  \n",
            "4                           [95, 96, 97, 98, 24, 99]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding and Truncation"
      ],
      "metadata": {
        "id": "LvsQyh2AmSla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_and_truncate(sequence, max_length, pad_value=0):\n",
        "  sequence = sequence[:max_length]\n",
        "  sequence += [pad_value]*(max_length - len(sequence))\n",
        "  return sequence\n",
        "\n",
        "MAX_LEN = 120\n",
        "train_subset['padded_ids'] = train_subset['token_ids'].apply(lambda seq: pad_and_truncate(seq, max_length=MAX_LEN, pad_value=vocab['<PAD>']))\n",
        "print(train_subset[['token_ids', 'padded_ids']].head())\n",
        "\n",
        "lengths = train_subset['padded_ids'].apply(len)\n",
        "print(\"Unique sequence lengths after padding:\", lengths.unique())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-vRr1k-mXcM",
        "outputId": "b4bb5fa5-8ad5-427e-e016-cd85e208d4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           token_ids  \\\n",
            "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
            "1        [1, 28, 29, 30, 26, 31, 32, 33, 23, 34, 35]   \n",
            "2  [36, 37, 26, 38, 39, 40, 41, 42, 43, 44, 45, 4...   \n",
            "3  [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 6...   \n",
            "4                           [95, 96, 97, 98, 24, 99]   \n",
            "\n",
            "                                          padded_ids  \n",
            "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
            "1  [1, 28, 29, 30, 26, 31, 32, 33, 23, 34, 35, 0,...  \n",
            "2  [36, 37, 26, 38, 39, 40, 41, 42, 43, 44, 45, 4...  \n",
            "3  [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 6...  \n",
            "4  [95, 96, 97, 98, 24, 99, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "Unique sequence lengths after padding: [120]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n"
      ],
      "metadata": {
        "id": "PbZDoBspttXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set size of each word vector\n",
        "embedding_dim = 100\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Get the index reserved for padding\n",
        "pad_idx = vocab['<PAD>']\n",
        "\n",
        "# Create embedding layer that maps word indices to embedding vectors\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "# Select sample sequence frpm the processed training data\n",
        "sample_sequence = torch.LongTensor(train_subset['padded_ids'].iloc[0])\n",
        "\n",
        "# Pass sample sequence through the embedding layer to get its embeddings\n",
        "sample_embeddings = embedding_layer(sample_sequence)\n",
        "\n",
        "print(\"Sample embeddings shape:\", sample_embeddings.shape)\n",
        "print(\"Embedding matrix shape:\", embedding_layer.weight.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq_2J4uvtyOE",
        "outputId": "d38e31cb-d4aa-4f97-8760-cc3119fff5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample embeddings shape: torch.Size([120, 100])\n",
            "Embedding matrix shape: torch.Size([62476, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward Layer with LSTM and Dropout"
      ],
      "metadata": {
        "id": "kDMqifKdtlq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import final\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "    super(FeedForward, self).__init__()\n",
        "\n",
        "    # Map word indices to dense vectors\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "    # Create bidirectional LSTM layer - with outputs concatenated from forward and backward passes\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "    # Dropout layer for regularization - Drop 50% of neurons during training\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # Fully connected layer mapping from hidden dimension to output dimension\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, text):\n",
        "    # Pass word indices through embedding layer\n",
        "    embedded = self.embedding(text)\n",
        "\n",
        "    # Pass embedded text through LSTM\n",
        "    lstm_output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "    # Select last hidden state from LSTM\n",
        "    f_hidden = hidden[-1]\n",
        "\n",
        "    # Apply dropout to the selected hidden state - Help reduce overfitting\n",
        "    drop = self.dropout(f_hidden)\n",
        "\n",
        "    # Pass dropped out features through the fully connected layer to get final output\n",
        "    fc_output = self.fc(f_hidden)\n",
        "\n",
        "\n",
        "    # Return logits for each class\n",
        "    return fc_output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5p1YiChxtkVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Params\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 64\n",
        "output_dim = 6\n",
        "pad_idx = vocab['<PAD>']\n",
        "\n",
        "# Create instance of the FeedForward model\n",
        "model = FeedForward(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Retrieve first padded sequence, convert to torch LongTensor, add an extra dimension at the start to represent the batch\n",
        "sample_sequence = torch.LongTensor(train_subset['padded_ids'].iloc[0]).unsqueeze(0)\n",
        "\n",
        "# Pass sample sequence through the model to generate output logits\n",
        "output_tensor = model(sample_sequence)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPxxsw8Lvj3A",
        "outputId": "aa8aebba-d6f8-4fd4-d975-6d2320ec477f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForward(\n",
            "  (embedding): Embedding(62476, 100, padding_idx=0)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "Output tensor shape: torch.Size([1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "zNiFWPLQzKKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ToxicDataset(Dataset):\n",
        "  def __init__(self, dataFrame, text_col='padded_ids', label_cols=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
        "    self.dataFrame = dataFrame\n",
        "    self.text_col = text_col\n",
        "    self.label_cols = label_cols\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataFrame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get a row from the DataFrame at given index\n",
        "    row = self.dataFrame.iloc[idx]\n",
        "\n",
        "    # Convert the padded text (list of token IDs) into a torch LongTensor\n",
        "    input_ids = torch.LongTensor(row[self.text_col])\n",
        "\n",
        "    # Convert the label columns to a float32 tensor\n",
        "    labels = torch.tensor(row[self.label_cols].astype('float32').values, dtype=torch.float32)\n",
        "\n",
        "    return input_ids, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "MY-eVM2GzIzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom dataset from the training subset using the ToxicDataset class\n",
        "train_dataset = ToxicDataset(train_subset)\n",
        "\n",
        "# Create a DataLoader to iterate over the training set\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate FeedForward model using defined parameters\n",
        "model = FeedForward(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx)\n",
        "\n",
        "# Define the loss function - Binary Cross Entropy (BCE)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Initialize Adam optimizer with learning rate of 1e-3 and weight decay of 1e-5 (Help prevent overfitting)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZaBD7qO0QFR",
        "outputId": "090b52e2-ad53-4104-e089-4e8f6952136c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedForward(\n",
              "  (embedding): Embedding(62476, 100, padding_idx=0)\n",
              "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split it into train and validation portions with 80/20 split\n",
        "train_df, val_df = train_test_split(train_subset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create separate dataset objects\n",
        "train_dataset = ToxicDataset(train_df)\n",
        "val_dataset = ToxicDataset(val_df)\n",
        "\n",
        "# Create dataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "As15d0gI_qpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import math, copy\n",
        "\n",
        "#Define labels, thresholds, and class counts\n",
        "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "class_counts = {\n",
        "    \"toxic\": 15294,\n",
        "    \"severe_toxic\": 1595,\n",
        "    \"obscene\": 8449,\n",
        "    \"threat\": 478,\n",
        "    \"insult\": 7877,\n",
        "    \"identity_hate\": 1405\n",
        "}\n",
        "\n",
        "# Thresholds for converting predicted probabilities into binary labels\n",
        "optimized_thresholds = {\n",
        "    \"toxic\": 0.5082,\n",
        "    \"severe_toxic\": 0.5408,\n",
        "    \"obscene\": 0.6551,\n",
        "    \"threat\": 0.1,\n",
        "    \"insult\": 0.5082,\n",
        "    \"identity_hate\": 0.1\n",
        "}\n",
        "\n",
        "# Compute weights using the square root ratio relative to the most frequent class\n",
        "max_count = max(class_counts.values())\n",
        "weights = [math.sqrt(max_count / class_counts[lbl]) for lbl in label_cols]\n",
        "print(\"Computed weights:\", {lbl: round(w, 2) for lbl, w in zip(label_cols, weights)})\n",
        "\n",
        "# Convert the optimized thresholds into a numpy array\n",
        "optimized_thresh_array = np.array([optimized_thresholds[lbl] for lbl in label_cols])\n",
        "print(\"Optimized thresholds array:\", optimized_thresh_array)\n",
        "\n",
        "# Create a tensor for the weights, and use it in BCEWithLogitsLoss for multi-label classification\n",
        "pos_weight = torch.tensor(weights, device=device)  # tensor of shape (num_classes,)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, dataloader, criterion, device, thresh=0.5):\n",
        "    \"\"\"\n",
        "    Runs the model on the validation set and returns:\n",
        "      - average loss\n",
        "      - per‑class F1 scores\n",
        "      - macro‑averaged F1 score\n",
        "      - raw probabilities (all_probs)\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss, all_probs, all_labels = 0.0, [], []\n",
        "\n",
        "    # No gradients needed for evalulation\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)  # Accumlate loss weighted by batch size\n",
        "            probs = torch.sigmoid(logits)             # Compute probabilities from logits\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(dataloader.dataset) # Average loss over the entire validation set\n",
        "\n",
        "    # Concatenate all batch outputs into single arrays\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Convert probabilities to binary predictions using thresholds\n",
        "    if np.isscalar(thresh):\n",
        "        bin_preds = (all_probs >= thresh).astype(int)\n",
        "    else:\n",
        "        thresh = np.asarray(thresh)\n",
        "        bin_preds = (all_probs >= thresh).astype(int)\n",
        "\n",
        "    # Compute F1 scores for each label\n",
        "    f1_per_class = [f1_score(all_labels[:, i], bin_preds[:, i], zero_division=0)\n",
        "                    for i in range(all_labels.shape[1])]\n",
        "    f1_macro = np.mean(f1_per_class)\n",
        "\n",
        "    return val_loss, f1_per_class, f1_macro, all_probs, all_labels\n",
        "\n",
        "# Main Training Loop\n",
        "num_epochs      = 8\n",
        "patience        = 2  # Stop if no improvement for 2 consecutive epochs\n",
        "checkpoint_path = \"best_macroF1_weighted.pt\"\n",
        "delta           = 1e-4  # Min. improvement threshold\n",
        "\n",
        "best_macro_f1     = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Loop over each epoch\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0  # Accumulate loss over batches\n",
        "\n",
        "    # Loop over training data\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() # Clear previous gradients\n",
        "\n",
        "        logits = model(inputs)  # Get model outputs\n",
        "        loss = criterion(logits, labels)  # Compute loss\n",
        "        loss.backward() # Backpropagate\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)`# Accumulate weighted loss\n",
        "\n",
        "    # Compute average training loss\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # Evaluate model on validation set and capture predictions\n",
        "    val_loss, f1_per_class, f1_macro, all_probs, all_labels = evaluate(model, val_dataloader, criterion, device, thresh=optimized_thresh_array)\n",
        "\n",
        "\n",
        "    # Save the best model, based on macro F1 - Use early stopping\n",
        "    if f1_macro > best_macro_f1 + delta:\n",
        "        best_macro_f1 = f1_macro\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(\"New best model saved (macro‑F1)\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No macro‑F1 improvement for {epochs_no_improve} epoch(s)\")\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    print(f\"  Train Loss  : {train_loss:.4f}\")\n",
        "    print(f\"  Val   Loss  : {val_loss:.4f}\")\n",
        "    print(f\"  F1 per class: {np.round(f1_per_class, 4).tolist()}\")\n",
        "    print(f\"  F1 macro    : {f1_macro:.4f}\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(\"Early stopping on macro‑F1.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4-MZne64fEl",
        "outputId": "8bc46114-f78e-45a4-fe5a-365ecc336bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed weights: {'toxic': 1.0, 'severe_toxic': 3.1, 'obscene': 1.35, 'threat': 5.66, 'insult': 1.39, 'identity_hate': 3.3}\n",
            "Optimized thresholds array: [0.5082 0.5408 0.6551 0.1    0.5082 0.1   ]\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 1/8\n",
            "  Train Loss  : 0.1077\n",
            "  Val   Loss  : 0.0772\n",
            "  F1 per class: [0.7638, 0.4088, 0.7504, 0.0855, 0.6864, 0.1962]\n",
            "  F1 macro    : 0.4818\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 2/8\n",
            "  Train Loss  : 0.0703\n",
            "  Val   Loss  : 0.0680\n",
            "  F1 per class: [0.7629, 0.4989, 0.7951, 0.1592, 0.6795, 0.2573]\n",
            "  F1 macro    : 0.5255\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 3/8\n",
            "  Train Loss  : 0.0622\n",
            "  Val   Loss  : 0.0651\n",
            "  F1 per class: [0.7787, 0.5105, 0.7885, 0.1176, 0.7128, 0.2696]\n",
            "  F1 macro    : 0.5296\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 4/8\n",
            "  Train Loss  : 0.0576\n",
            "  Val   Loss  : 0.0637\n",
            "  F1 per class: [0.7847, 0.4866, 0.7724, 0.2197, 0.7099, 0.3587]\n",
            "  F1 macro    : 0.5554\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 5/8\n",
            "  Train Loss  : 0.0539\n",
            "  Val   Loss  : 0.0629\n",
            "  F1 per class: [0.783, 0.4971, 0.7935, 0.2542, 0.7184, 0.3599]\n",
            "  F1 macro    : 0.5677\n",
            "No macro‑F1 improvement for 1 epoch(s)\n",
            "Epoch 6/8\n",
            "  Train Loss  : 0.0516\n",
            "  Val   Loss  : 0.0628\n",
            "  F1 per class: [0.7916, 0.4943, 0.8051, 0.2247, 0.7202, 0.3162]\n",
            "  F1 macro    : 0.5587\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 7/8\n",
            "  Train Loss  : 0.0489\n",
            "  Val   Loss  : 0.0620\n",
            "  F1 per class: [0.7954, 0.5083, 0.8163, 0.2593, 0.7329, 0.361]\n",
            "  F1 macro    : 0.5788\n",
            "New best model saved (macro‑F1)\n",
            "Epoch 8/8\n",
            "  Train Loss  : 0.0471\n",
            "  Val   Loss  : 0.0643\n",
            "  F1 per class: [0.7766, 0.5087, 0.7982, 0.2931, 0.7161, 0.3933]\n",
            "  F1 macro    : 0.5810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.save(\"all_probs.npy\", all_probs)\n",
        "np.save(\"all_labels.npy\", all_labels)"
      ],
      "metadata": {
        "id": "VCIeT1f5Dlgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_p = np.load(\"all_probs.npy\")\n",
        "all_l = np.load(\"all_labels.npy\")\n",
        "\n",
        "print(\"Shape of all_probs:\", all_probs.shape)\n",
        "print(\"Shape of all_labels:\", all_labels.shape)\n",
        "print(\"Preview of all_probs:\")\n",
        "print(all_probs[:5])\n",
        "print(\"Preview of all_labels:\")\n",
        "print(all_labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UwivYcvENsE",
        "outputId": "56d90bb5-1d68-4c14-e159-d525673eeed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all_probs: (31915, 6)\n",
            "Shape of all_labels: (31915, 6)\n",
            "Preview of all_probs:\n",
            "[[2.9861578e-01 1.2028109e-03 4.2501405e-02 1.2003662e-03 2.0981105e-01\n",
            "  2.7093254e-02]\n",
            " [1.7251348e-03 4.9220725e-05 3.2845960e-04 1.2406283e-04 2.6598977e-04\n",
            "  1.5332771e-04]\n",
            " [3.4004752e-02 4.9237802e-04 7.1031009e-03 1.5335761e-03 9.2546688e-03\n",
            "  2.8343578e-03]\n",
            " [1.3090563e-03 4.3905198e-05 3.5629544e-04 1.2484952e-04 1.2880859e-04\n",
            "  7.3169460e-05]\n",
            " [1.4373098e-03 1.8635601e-05 2.3001932e-04 6.0563147e-05 1.7523204e-04\n",
            "  2.3720022e-04]]\n",
            "Preview of all_labels:\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        ")\n",
        "from pathlib import Path\n",
        "\n",
        "# Define labels and thresholds\n",
        "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "optimized_thresholds = {\n",
        "    \"toxic\": 0.5082,\n",
        "    \"severe_toxic\": 0.5408,\n",
        "    \"obscene\": 0.6551,\n",
        "    \"threat\": 0.1,\n",
        "    \"insult\": 0.5082,\n",
        "    \"identity_hate\": 0.1\n",
        "}\n",
        "optimized_thresh_array = np.array([optimized_thresholds[lbl] for lbl in label_cols])\n",
        "print(\"Optimized thresholds array:\", optimized_thresh_array)\n",
        "\n",
        "# Binarize predictions using thresholds\n",
        "bin_preds = (all_probs >= optimized_thresh_array).astype(int)\n",
        "\n",
        "# Compute Per‑label Precision, Recall, and F1 metrics\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(all_labels, bin_preds, average=None, zero_division=0)\n",
        "macro_f1 = np.mean(f1)\n",
        "\n",
        "# Micro averaged metrics\n",
        "micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(\n",
        "    all_labels.ravel(), bin_preds.ravel(), average=\"micro\", zero_division=0\n",
        ")\n",
        "\n",
        "subset_acc = accuracy_score(all_labels.tolist(), bin_preds.tolist())\n",
        "roc_auc_macro = roc_auc_score(all_labels, all_probs, average=\"macro\")\n",
        "pr_auc_macro = average_precision_score(all_labels, all_probs, average=\"macro\")\n",
        "\n",
        "# Compute all label Confusion Matrices\n",
        "conf_matrices = {}\n",
        "jacc_per_label = []\n",
        "for i, lbl in enumerate(label_cols):\n",
        "    cm = confusion_matrix(all_labels[:, i], bin_preds[:, i])\n",
        "    conf_matrices[lbl] = cm\n",
        "    print(f\"Confusion matrix for {lbl}:\")\n",
        "    print(cm)\n",
        "    print()\n",
        "\n",
        "\n",
        "print(\"\\n===== Evaluation Metrics =====\")\n",
        "for i, lbl in enumerate(label_cols):\n",
        "    print(f\"{lbl:15s}  Precision: {prec[i]:.3f}  Recall: {rec[i]:.3f}  F1: {f1[i]:.3f}\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(f\"Macro‑F1           : {macro_f1:.4f}\")\n",
        "print(f\"Micro‑F1           : {micro_f1:.4f}\")\n",
        "print(f\"Subset accuracy    : {subset_acc:.4f}\")\n",
        "print(f\"ROC‑AUC (macro)    : {roc_auc_macro:.4f}\")\n",
        "print(f\"PR‑AUC  (macro)    : {pr_auc_macro:.4f}\")\n",
        "print(\"=================================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5awGWlrBb1L",
        "outputId": "8b90764a-dfee-41df-fb25-6e68c9576318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized thresholds array: [0.5082 0.5408 0.6551 0.1    0.5082 0.1   ]\n",
            "Confusion matrix for toxic:\n",
            "[[28512   347]\n",
            " [  896  2160]]\n",
            "\n",
            "Confusion matrix for severe_toxic:\n",
            "[[31399   195]\n",
            " [  145   176]]\n",
            "\n",
            "Confusion matrix for obscene:\n",
            "[[30027   173]\n",
            " [  461  1254]]\n",
            "\n",
            "Confusion matrix for threat:\n",
            "[[31583   258]\n",
            " [   17    57]]\n",
            "\n",
            "Confusion matrix for insult:\n",
            "[[29794   507]\n",
            " [  431  1183]]\n",
            "\n",
            "Confusion matrix for identity_hate:\n",
            "[[31057   564]\n",
            " [   84   210]]\n",
            "\n",
            "\n",
            "===== Evaluation Metrics =====\n",
            "toxic            Precision: 0.862  Recall: 0.707  F1: 0.777\n",
            "severe_toxic     Precision: 0.474  Recall: 0.548  F1: 0.509\n",
            "obscene          Precision: 0.879  Recall: 0.731  F1: 0.798\n",
            "threat           Precision: 0.181  Recall: 0.770  F1: 0.293\n",
            "insult           Precision: 0.700  Recall: 0.733  F1: 0.716\n",
            "identity_hate    Precision: 0.271  Recall: 0.714  F1: 0.393\n",
            "-------------------------------------------------\n",
            "Macro‑F1           : 0.5810\n",
            "Micro‑F1           : 0.9787\n",
            "Subset accuracy    : 0.9104\n",
            "ROC‑AUC (macro)    : 0.9811\n",
            "PR‑AUC  (macro)    : 0.6519\n",
            "=================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate word embeddings to show similar words"
      ],
      "metadata": {
        "id": "0e7VUSv1BTCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract the embedding weights from the model's embedding layer\n",
        "model.eval()\n",
        "embedding_weights = model.embedding.weight.data.cpu().numpy()\n",
        "\n",
        "# Build an inverse vocabulary (mapping index to word).\n",
        "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute the cosine similarity between two vectors.\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def find_similar_words(query_word, embedding_weights, vocab, inv_vocab, top_n=10):\n",
        "    \"\"\"Find the top_n words most similar to the query_word.\"\"\"\n",
        "    if query_word not in vocab:\n",
        "        print(f\"'{query_word}' not found in the vocabulary.\")\n",
        "        return []\n",
        "    query_idx = vocab[query_word]\n",
        "    query_vec = embedding_weights[query_idx]\n",
        "\n",
        "    # Compute cosine similarity for every word in the vocabulary\n",
        "    similarities = []\n",
        "    for idx, vec in enumerate(embedding_weights):\n",
        "        sim = cosine_similarity(query_vec, vec)\n",
        "        similarities.append((inv_vocab[idx], sim))\n",
        "\n",
        "    # Sort by similarity, highest first\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "    similar_words = [(word, sim) for word, sim in similarities if word != query_word][1:top_n]\n",
        "    return similar_words\n",
        "\n",
        "# Find words similar to \"sucker\"\n",
        "similar_words = find_similar_words(\"sucker\", embedding_weights, vocab, inv_vocab, top_n=20)\n",
        "print(\"Words similar to 'sucker':\")\n",
        "for word, sim in similar_words:\n",
        "    print(f\"{word}: {sim:.4f}\")\n"
      ],
      "metadata": {
        "id": "bMubqwjIBRwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda9761d-8ed6-4d51-8460-a339456259d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'sucker':\n",
            "badaga: 0.3560\n",
            "ramped: 0.3459\n",
            "provocateur: 0.3375\n",
            "chicken: 0.3367\n",
            "stupid: 0.3361\n",
            "marathonios: 0.3318\n",
            "sizzling: 0.3305\n",
            "vicepresidential: 0.3296\n",
            "infactual: 0.3289\n",
            "twenties: 0.3268\n",
            "ayoubmalouk: 0.3264\n",
            "nao: 0.3208\n",
            "moron: 0.3207\n",
            "thong: 0.3205\n",
            "dogshit: 0.3185\n",
            "vcard: 0.3181\n",
            "transitioned: 0.3150\n",
            "ewhc: 0.3130\n",
            "tormenting: 0.3130\n"
          ]
        }
      ]
    }
  ]
}