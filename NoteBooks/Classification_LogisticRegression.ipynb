{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Processing"
      ],
      "metadata": {
        "id": "pkNZ0Z1OwAhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0P8qUYTva1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e849e9-be06-425b-a874-327587b80f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Training dataset shape: (159571, 8)\n",
            "Test dataset shape: (153164, 2)\n",
            "                                        comment_text  \\\n",
            "0  Explanation\\nWhy the edits made under my usern...   \n",
            "1  D'aww! He matches this background colour I'm s...   \n",
            "2  Hey man, I'm really not trying to edit war. It...   \n",
            "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
            "4  You, sir, are my hero. Any chance you remember...   \n",
            "\n",
            "                                       clean_comment  \n",
            "0  explanation why the edits made under my userna...  \n",
            "1  daww he matches this background colour im seem...  \n",
            "2  hey man im really not trying to edit war its j...  \n",
            "3  more i cant make any real suggestions on impro...  \n",
            "4  you sir are my hero any chance you remember wh...  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to the zip files in Drive\n",
        "zip_dir = '/content/drive/MyDrive/jigsaw-toxic-comment-classification-challenge/'\n",
        "\n",
        "# Unzip training and testing zips\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_dir + 'train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "with zipfile.ZipFile(zip_dir + 'test.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "print(\"Training dataset shape:\", train_df.shape)\n",
        "print(\"Test dataset shape:\", test_df.shape)\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)                  # Remove emails\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)                 # Remove special characters and numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()             # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Clean the comments\n",
        "train_df['clean_comment'] = train_df['comment_text'].apply(clean_text)\n",
        "test_df['clean_comment'] = test_df['comment_text'].apply(clean_text)\n",
        "\n",
        "print(train_df[['comment_text', 'clean_comment']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction with TF-IDF"
      ],
      "metadata": {
        "id": "m3rBZI_BwC7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TF‑IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(train_df['clean_comment'])\n",
        "X_test = vectorizer.transform(test_df['clean_comment'])\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "\n",
        "# Define toxicity labels\n",
        "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "y_train = train_df[label_cols].values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09xe7nwLwIT5",
        "outputId": "1c30d9c6-663b-4303-e91a-9dc5de46c744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (159571, 10000)\n",
            "Shape of X_test: (153164, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Validation Split and Logistic Regression Training"
      ],
      "metadata": {
        "id": "dPMSnGghwJzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Split the data - 80/20 Split\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up a multi-label classifier using Logistic Regression wrapped in One-vs-Rest\n",
        "lr_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
        "lr_clf.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Predict probabilities and hard labels on the validation set\n",
        "y_val_prob = lr_clf.predict_proba(X_val)\n",
        "y_val_pred = lr_clf.predict(X_val)\n",
        "\n",
        "print(\"Validation predictions shape:\", y_val_pred.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_pjm8ciwMxN",
        "outputId": "acfdf3e3-9577-47cc-809b-95ce7ac4c9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation predictions shape: (31915, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "pWluClNpwReS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# Define labels and thresholds\n",
        "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "optimized_thresholds = {\n",
        "    \"toxic\": 0.5082,\n",
        "    \"severe_toxic\": 0.5408,\n",
        "    \"obscene\": 0.6551,\n",
        "    \"threat\": 0.1,\n",
        "    \"insult\": 0.5082,\n",
        "    \"identity_hate\": 0.1\n",
        "}\n",
        "optimized_thresh_array = np.array([optimized_thresholds[lbl] for lbl in label_cols])\n",
        "print(\"Optimized thresholds array:\", optimized_thresh_array)\n",
        "\n",
        "# Use the predicted probabilities and ground truth labels from the validation set\n",
        "all_probs = y_val_prob  # Predicted probabilities\n",
        "all_labels = y_val      # Ground truth labels\n",
        "\n",
        "# Binarize predictions using thresholds - Apply each threshold per label to obtain binary predictions\n",
        "bin_preds = (all_probs >= optimized_thresh_array).astype(int)\n",
        "\n",
        "# Compute per‑label Precision, Recall, and F1 scores\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(all_labels, bin_preds, average=None, zero_division=0)\n",
        "macro_f1 = np.mean(f1)\n",
        "\n",
        "# Compute micro‑averaged metrics\n",
        "micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(\n",
        "    all_labels.ravel(), bin_preds.ravel(), average=\"micro\", zero_division=0\n",
        ")\n",
        "\n",
        "# Compute subset accuracy\n",
        "subset_acc = accuracy_score(all_labels, bin_preds)\n",
        "\n",
        "# Compute ROC‑AUC and PR‑AUC\n",
        "roc_auc_macro = roc_auc_score(all_labels, all_probs, average=\"macro\")\n",
        "pr_auc_macro = average_precision_score(all_labels, all_probs, average=\"macro\")\n",
        "\n",
        "# Compute and print per‑label Confusion Matrices\n",
        "conf_matrices = {}\n",
        "for i, lbl in enumerate(label_cols):\n",
        "    cm = confusion_matrix(all_labels[:, i], bin_preds[:, i])\n",
        "    conf_matrices[lbl] = cm\n",
        "    print(f\"Confusion matrix for {lbl}:\")\n",
        "    print(cm)\n",
        "    print()\n",
        "\n",
        "# Print Report\n",
        "print(\"\\n===== Evaluation Metrics =====\")\n",
        "for i, lbl in enumerate(label_cols):\n",
        "    print(f\"{lbl:15s}  Precision: {prec[i]:.3f}  Recall: {rec[i]:.3f}  F1: {f1[i]:.3f}\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(f\"Macro‑F1           : {macro_f1:.4f}\")\n",
        "print(f\"Micro‑F1           : {micro_f1:.4f}\")\n",
        "print(f\"Subset accuracy    : {subset_acc:.4f}\")\n",
        "print(f\"ROC‑AUC (macro)    : {roc_auc_macro:.4f}\")\n",
        "print(f\"PR‑AUC  (macro)    : {pr_auc_macro:.4f}\")\n",
        "print(\"=================================================\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b4FrXDDwUeK",
        "outputId": "958fdcef-7731-47be-aa1f-ee9a43cc867d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized thresholds array: [0.5082 0.5408 0.6551 0.1    0.5082 0.1   ]\n",
            "Confusion matrix for toxic:\n",
            "[[28665   194]\n",
            " [ 1221  1835]]\n",
            "\n",
            "Confusion matrix for severe_toxic:\n",
            "[[31539    55]\n",
            " [  240    81]]\n",
            "\n",
            "Confusion matrix for obscene:\n",
            "[[30130    70]\n",
            " [  763   952]]\n",
            "\n",
            "Confusion matrix for threat:\n",
            "[[31785    56]\n",
            " [   51    23]]\n",
            "\n",
            "Confusion matrix for insult:\n",
            "[[30138   163]\n",
            " [  812   802]]\n",
            "\n",
            "Confusion matrix for identity_hate:\n",
            "[[31403   218]\n",
            " [  158   136]]\n",
            "\n",
            "\n",
            "===== Evaluation Metrics =====\n",
            "toxic            Precision: 0.904  Recall: 0.600  F1: 0.722\n",
            "severe_toxic     Precision: 0.596  Recall: 0.252  F1: 0.354\n",
            "obscene          Precision: 0.932  Recall: 0.555  F1: 0.696\n",
            "threat           Precision: 0.291  Recall: 0.311  F1: 0.301\n",
            "insult           Precision: 0.831  Recall: 0.497  F1: 0.622\n",
            "identity_hate    Precision: 0.384  Recall: 0.463  F1: 0.420\n",
            "-------------------------------------------------\n",
            "Macro‑F1           : 0.5190\n",
            "Micro‑F1           : 0.9791\n",
            "Subset accuracy    : 0.9146\n",
            "ROC‑AUC (macro)    : 0.9742\n",
            "PR‑AUC  (macro)    : 0.6002\n",
            "=================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}